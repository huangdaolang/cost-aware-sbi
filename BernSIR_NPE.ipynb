{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed06cb3d",
   "metadata": {},
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sbi import utils as utils\n",
    "from sbi import analysis as analysis\n",
    "from cost_aware_snpe_c import CostAwareSNPE_C\n",
    "from sbi.inference.snpe.snpe_c import SNPE_C\n",
    "from sbi.utils.torchutils import *\n",
    "from sbi.utils import process_prior\n",
    "from sbi.utils.user_input_checks import *\n",
    "from simulators import bernoulli_sir\n",
    "\n",
    "from hydra import compose, initialize\n",
    "import hydra\n",
    "\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize(config_path=\"configs\", version_base=None)\n",
    "cfg = compose(config_name=\"train\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77dc6ef3",
   "metadata": {},
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams.update({\n",
    "    'font.family' : 'times',\n",
    "    'font.size' : 14.0,\n",
    "    'lines.linewidth' : 2,\n",
    "    'lines.antialiased' : True,\n",
    "    'axes.facecolor': 'fdfdfd',\n",
    "    'axes.edgecolor': '777777',\n",
    "    'axes.linewidth' : 1,\n",
    "    'axes.titlesize' : 'medium',\n",
    "    'axes.labelsize' : 'medium',\n",
    "    'axes.axisbelow' : True,\n",
    "    'xtick.major.size'     : 0,      # major tick size in points\n",
    "    'xtick.minor.size'     : 0,      # minor tick size in points\n",
    "    'xtick.major.pad'      : 6,      # distance to major tick label in points\n",
    "    'xtick.minor.pad'      : 6,      # distance to the minor tick label in points\n",
    "    'xtick.color'          : '333333', # color of the tick labels\n",
    "    'xtick.labelsize'      : 'medium', # fontsize of the tick labels\n",
    "    'xtick.direction'      : 'in',     # direction: in or out\n",
    "    'ytick.major.size'     : 0,      # major tick size in points\n",
    "    'ytick.minor.size'     : 0,      # minor tick size in points\n",
    "    'ytick.major.pad'      : 6,      # distance to major tick label in points\n",
    "    'ytick.minor.pad'      : 6,      # distance to the minor tick label in points\n",
    "    'ytick.color'          : '333333', # color of the tick labels\n",
    "    'ytick.labelsize'      : 'medium', # fontsize of the tick labels\n",
    "    'ytick.direction'      : 'in',     # direction: in or out\n",
    "    'axes.grid' : False,\n",
    "    'grid.alpha' : 0.3,\n",
    "    'grid.linewidth' : 1,\n",
    "    'legend.fancybox' : True,\n",
    "    'legend.fontsize' : 'Small',\n",
    "    'figure.figsize' : (2.5, 2.5),\n",
    "    'figure.facecolor' : '1.0',\n",
    "    'figure.edgecolor' : '0.5',\n",
    "    'hatch.linewidth' : 0.1,\n",
    "    'text.usetex' : True\n",
    "    })\n",
    "\n",
    "color_map = {'green': '#009E60', 'orange': '#C04000',\n",
    "              'blue': '#00416A', 'black':'#3A3B3C',\n",
    "              'purple': '#843B62', 'red': '#C41E3A'}\n",
    "\n",
    "\n",
    "plt.rcParams['text.latex.preamble'] = r'\\usepackage{times}'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b9f2164",
   "metadata": {},
   "source": [
    "def MMD_unweighted(x, y, lengthscale):\n",
    "    \"\"\" Approximates the squared MMD between samples x_i ~ P and y_i ~ Q\n",
    "    \"\"\"\n",
    "\n",
    "    m = x.shape[0]\n",
    "    n = y.shape[0]\n",
    "\n",
    "    z = torch.cat((x, y), dim=0)\n",
    "\n",
    "    K = kernel_matrix(z, z, lengthscale)\n",
    "\n",
    "    kxx = K[0:m, 0:m]\n",
    "    kyy = K[m:(m + n), m:(m + n)]\n",
    "    kxy = K[0:m, m:(m + n)]\n",
    "\n",
    "    return (1 / m ** 2) * torch.sum(kxx) - (2 / (m * n)) * torch.sum(kxy) + (1 / n ** 2) * torch.sum(kyy)\n",
    "\n",
    "\n",
    "def median_heuristic(y):\n",
    "    a = torch.cdist(y, y)**2\n",
    "    return torch.sqrt(torch.median(a / 2))\n",
    "\n",
    "\n",
    "def kernel_matrix(x, y, l):\n",
    "    d = torch.cdist(x, y)**2\n",
    "\n",
    "    kernel = torch.exp(-(1 / (2 * l ** 2)) * d)\n",
    "\n",
    "    return kernel"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f456b12e",
   "metadata": {},
   "source": [
    "def calc_acc_prob(gp, likelihood, theta, prior_start, k):\n",
    "\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        cost = likelihood(model(theta)).mean\n",
    "\n",
    "        lower_cost = likelihood(model(prior_start)).mean\n",
    "    return (lower_cost ** k) / (cost**k)\n",
    "\n",
    "class GP(ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GP, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = ConstantMean()\n",
    "        self.covar_module = ScaleKernel(RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x, covar_x)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "55625add",
   "metadata": {},
   "source": [
    "# Bernoulli SIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83bc1dbe",
   "metadata": {},
   "source": [
    "bern_sir = bernoulli_sir.BernSIR()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "55006939",
   "metadata": {},
   "source": [
    "## data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35938d6f",
   "metadata": {},
   "source": [
    "# DON'T NEED TO RUN\n",
    "N = 50000\n",
    "\n",
    "bern_sir_theta_npe_large = bern_sir.sample_theta([N]).reshape(-1, bern_sir.theta_dim)\n",
    "bern_sir_x_npe_large = torch.empty([N, bern_sir.x_dim])\n",
    "for i in range(N):\n",
    "    bern_sir_x_npe_large[i, :] = bern_sir(bern_sir_theta_npe_large[i])\n",
    "\n",
    "torch.save(bern_sir_x_npe_large, \"data/bern_sir_x_npe_large.pt\")\n",
    "torch.save(bern_sir_theta_npe_large, \"data/bern_sir_theta_npe_large.pt\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bfeb930",
   "metadata": {},
   "source": [
    "# Use triton to generate data\n",
    "N = 50000\n",
    "\n",
    "bern_sir_theta_npe_large = bern_sir.sample_theta([N]).reshape(-1, bern_sir.theta_dim)\n",
    "bern_sir_x_npe_large = torch.empty([N, bern_sir.x_dim])\n",
    "for i in range(100):\n",
    "    bern_sir_x_npe_large[500*i: 500*(i+1), :] = torch.load(f\"data/bern_sir_large/bern_sir_x_npe_large_{i+1}.pt\")\n",
    "    bern_sir_theta_npe_large[500*i: 500*(i+1), :] = torch.load(f\"data/bern_sir_large/bern_sir_theta_npe_large_{i+1}.pt\")\n",
    "\n",
    "torch.save(bern_sir_x_npe_large, \"data/bern_sir_x_npe_large.pt\")\n",
    "torch.save(bern_sir_theta_npe_large, \"data/bern_sir_theta_npe_large.pt\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0dc2dfbf",
   "metadata": {},
   "source": [
    "# DON'T NEED TO RUN\n",
    "bern_sir_obs_theta = torch.tensor([0.5, 0.5, 0.5])\n",
    "bern_sir_obs_x = bern_sir(bern_sir_obs_theta)\n",
    "bern_sir_obs_x"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "05b486bb",
   "metadata": {},
   "source": [
    "# DON'T NEED TO RUN\n",
    "torch.save(bern_sir_obs_x, \"data/bern_sir_obs_x.pt\")\n",
    "torch.save(bern_sir_obs_theta, \"data/bern_sir_obs_theta.pt\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d156fb7e",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e004e1ca",
   "metadata": {},
   "source": [
    "bern_sir_x_npe_large = torch.load(\"data/bern_sir_x_npe_large.pt\")\n",
    "bern_sir_theta_npe_large = torch.load(\"data/bern_sir_theta_npe_large.pt\")\n",
    "\n",
    "bern_sir_obs_x = torch.load(\"data/bern_sir_obs_x.pt\")\n",
    "bern_sir_obs_theta = torch.load(\"data/bern_sir_obs_theta.pt\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1bd4256d",
   "metadata": {},
   "source": [
    "## Fit GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c08fe7f1",
   "metadata": {},
   "source": [
    "# for gp\n",
    "n_train_pair = 200\n",
    "X = bern_sir_theta_npe_large[:n_train_pair]\n",
    "n_rep = 20\n",
    "\n",
    "times_train = torch.zeros(n_train_pair)\n",
    "\n",
    "for i in range(n_train_pair):\n",
    "    st = time.time()\n",
    "    for _ in range(n_rep):\n",
    "        result = bern_sir(bern_sir_theta_npe_large[i])\n",
    "    et = time.time()\n",
    "    times_train[i] = et - st"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "172b38a7",
   "metadata": {},
   "source": [
    "# save GP training samples\n",
    "torch.save(X, \"data/bern_sir_gp_x.pt\")\n",
    "torch.save(times_train, \"data/bern_sir_gp_y.pt\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "887def4a",
   "metadata": {},
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = GP(X, times_train, likelihood)\n",
    "model.float()\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "mll = ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iterations = 500\n",
    "for i in range(training_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X.float())\n",
    "    loss = -mll(output, times_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Iter {i + 1}/{training_iterations} - Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'likelihood_state_dict': likelihood.state_dict()\n",
    "}, 'data/bern_sir_gp.pth')\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c685910e",
   "metadata": {},
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "N = 100\n",
    "\n",
    "beta = torch.linspace(0.1, 1, N)\n",
    "beta_fixed = 0.5\n",
    "\n",
    "\n",
    "gamma_fixed = 0.5\n",
    "gamma = torch.linspace(0.1, 1, N)\n",
    "\n",
    "p = torch.linspace(0.1, 1, N)\n",
    "p_fixed = 0.5\n",
    "\n",
    "beta_combined = beta_fixed * torch.ones_like(beta)\n",
    "gamma_combined = gamma_fixed * torch.ones_like(gamma)\n",
    "p_combined = p_fixed * torch.ones_like(p)\n",
    "\n",
    "combination_beta = torch.stack((beta, gamma_combined, p_combined), dim=1)\n",
    "combination_gamma = torch.stack((beta_combined, gamma, p_combined), dim=1)\n",
    "combination_p = torch.stack((beta_combined, gamma_combined, p), dim=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f71310c",
   "metadata": {},
   "source": [
    "times_vs_beta = torch.zeros(N)\n",
    "for i in range(N):\n",
    "    st = time.time()\n",
    "    for _ in range(n_rep):\n",
    "        result = bern_sir(combination_beta[i])\n",
    "    et = time.time()\n",
    "    times_vs_beta[i] = (et - st)\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    observed_pred = likelihood(model(combination_beta)).mean\n",
    "\n",
    "plt.figure(figsize=[5,5])\n",
    "plt.scatter(beta, observed_pred, label=\"GP\")\n",
    "plt.scatter(beta, times_vs_beta, label=\"True\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01c6d019",
   "metadata": {},
   "source": [
    "times_vs_gamma = torch.zeros(N)\n",
    "for i in range(N):\n",
    "    st = time.time()\n",
    "    for _ in range(n_rep):\n",
    "        result = bern_sir(combination_gamma[i])\n",
    "    et = time.time()\n",
    "    times_vs_gamma[i] = (et - st)\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    observed_pred = likelihood(model(combination_gamma)).mean\n",
    "\n",
    "plt.figure(figsize=[5,5])\n",
    "plt.scatter(gamma, observed_pred, label=\"GP\")\n",
    "plt.scatter(gamma, times_vs_gamma, label=\"True\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f7b61ee",
   "metadata": {},
   "source": [
    "times_vs_p = torch.zeros(N)\n",
    "for i in range(N):\n",
    "    st = time.time()\n",
    "    for _ in range(n_rep):\n",
    "        result = bern_sir(combination_p[i])\n",
    "    et = time.time()\n",
    "    times_vs_p[i] = (et - st)\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    observed_pred = likelihood(model(combination_p)).mean\n",
    "\n",
    "plt.figure(figsize=[5,5])\n",
    "plt.scatter(p, observed_pred, label=\"GP\")\n",
    "plt.scatter(p, times_vs_p, label=\"True\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5635c703",
   "metadata": {},
   "source": [
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    observed_pred = likelihood(model(torch.tensor([1.0, 0.1, 1.0]).reshape(1, -1))).mean\n",
    "    \n",
    "print(observed_pred)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "952af19d",
   "metadata": {},
   "source": [
    "state_dicts = torch.load('data/bern_sir_gp.pth')\n",
    "\n",
    "X = torch.load(\"data/bern_sir_gp_x.pt\")\n",
    "times_train = torch.load(\"data/bern_sir_gp_y.pt\")\n",
    "\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = GP(X, times_train, likelihood)\n",
    "\n",
    "model.load_state_dict(state_dicts['model_state_dict'])\n",
    "likelihood.load_state_dict(state_dicts['likelihood_state_dict'])\n",
    "\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "prior_start = torch.tensor([0.1, 1.0, 0.001]).reshape(-1, 3)\n",
    "k = 2\n",
    "\n",
    "num_sim = 5000\n",
    "theta_tilde = torch.zeros([num_sim, 3])\n",
    "count = 0\n",
    "while count < num_sim:\n",
    "    theta = bern_sir.sample_theta([1]).reshape(-1, 3)\n",
    "    if calc_acc_prob(model, likelihood, theta, prior_start, k) > torch.rand(1):\n",
    "        theta_tilde[count] = theta.reshape(-1)\n",
    "        count += 1"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "064af26f",
   "metadata": {},
   "source": [
    "w = likelihood(model(theta_tilde)).mean.detach() ** k"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b40f1b4c",
   "metadata": {},
   "source": [
    "theta_prior = bern_sir.sample_theta([5000]).reshape(-1, 3)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bf760676",
   "metadata": {},
   "source": [
    "plt.hist(theta_prior.detach().numpy()[:, 0], bins=10)\n",
    "plt.hist(theta_tilde.detach().numpy()[:, 0], bins=10)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7426368b",
   "metadata": {},
   "source": [
    "## CEG ESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24c93bd0",
   "metadata": {},
   "source": [
    "num_sim = 10000\n",
    "num_repeats = 2\n",
    "k = np.array([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]) # Exponent of the penaly function g(z) = z^k\n",
    "\n",
    "prior_start = torch.tensor([0.1, 1.0, 0.1]).reshape(-1, 3)\n",
    "\n",
    "ess_cost_aware = np.zeros([k.size, num_repeats])\n",
    "ceg = np.zeros([k.size, num_repeats])\n",
    "\n",
    "for ind in range(k.size):\n",
    "    for j in range(num_repeats):\n",
    "        \n",
    "        if ind == 0:\n",
    "            theta = bern_sir.sample_theta([num_sim])\n",
    "            ess_cost_aware[ind, j] = 1\n",
    "            ceg[ind, j] = 1\n",
    "        else:\n",
    "            # Sampling from cost-modified prior\n",
    "            theta_tilde = torch.zeros([num_sim, 3])\n",
    "            count = 0\n",
    "            while count < num_sim:\n",
    "                param_value = bern_sir.sample_theta([1]).reshape(-1, 3)\n",
    "                if calc_acc_prob(model, likelihood, param_value, prior_start, k[ind]) > torch.rand(1):\n",
    "                    theta_tilde[count] = param_value.reshape(-1)\n",
    "                    count += 1\n",
    "\n",
    "            w_u = likelihood(model(theta_tilde)).mean.detach() ** k[ind]   #self-normalised importance weights\n",
    "            \n",
    "            # Compute CEG\n",
    "            ceg[ind, j] = torch.mean(likelihood(model(theta)).mean.detach()) / torch.mean(likelihood(model(theta_tilde)).mean.detach())\n",
    "            # Compute ESS\n",
    "            ess_cost_aware[ind, j] = ((w_u.sum())**2 / torch.square(w_u).sum()) / num_sim\n",
    "    print(ind)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "368c11ce",
   "metadata": {},
   "source": [
    "ess_cost_aware * ceg"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e01a5d48",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "818064a6",
   "metadata": {},
   "source": [
    "# DON'T NEED TO RUN\n",
    "bern_sir_inference_npe_large = SNPE_C()\n",
    "bern_sir_nn_npe_large = bern_sir_inference_npe_large.append_simulations(\n",
    "    bern_sir_theta_npe_large, bern_sir_x_npe_large).train()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2c867d70",
   "metadata": {},
   "source": [
    "# DON'T NEED TO RUN\n",
    "prior, *_ = process_prior(bern_sir.prior)\n",
    "bern_sir_post_npe_large = bern_sir_inference_npe_large.build_posterior(bern_sir_nn_npe_large, prior=prior)\n",
    "bern_sir_samples_npe_large = bern_sir_post_npe_large.sample((1000,), x=bern_sir_obs_x)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bb69bfd8",
   "metadata": {},
   "source": [
    "# DON'T NEED TO RUN\n",
    "torch.save(bern_sir_samples_npe_large, \"data/bern_sir_post_reference.pt\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "48c2a51e",
   "metadata": {},
   "source": [
    "bern_sir_post_reference = torch.load(\"data/bern_sir_post_reference.pt\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b9f09f71",
   "metadata": {},
   "source": [
    "k = 0.5\n",
    "n_run = 100\n",
    "mmd_npe = torch.zeros([n_run])\n",
    "posterior_samples_npe = torch.zeros([n_run, 1000, 3])\n",
    "cost_npe = torch.zeros([n_run])\n",
    "mmd_canpe = torch.zeros([n_run])\n",
    "posterior_samples_canpe = torch.zeros([n_run, 1000, 3])\n",
    "cost_canpe = torch.zeros([n_run])\n",
    "\n",
    "cost_saved = torch.zeros([n_run])\n",
    "\n",
    "\n",
    "for i in range(n_run):\n",
    "    checkpoint_path = f\"sims/bern_sir/{k}/{i+1}/ckpt.tar\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    \n",
    "    posterior_npe = checkpoint[\"posterior_npe\"]\n",
    "    posterior_canpe = checkpoint[\"posterior_canpe\"]\n",
    "    \n",
    "\n",
    "#     posterior_samples_npe[i] = checkpoint[\"posterior_samples_npe\"]\n",
    "#     posterior_samples_canpe[i] = checkpoint[\"posterior_samples_canpe\"]\n",
    "    \n",
    "    posterior_samples_npe[i] = posterior_npe.sample((1000,), x=bern_sir_obs_x, show_progress_bars=False)\n",
    "    posterior_samples_canpe[i] = posterior_canpe.sample((1000,), x=bern_sir_obs_x, show_progress_bars=False)\n",
    "    \n",
    "    mmd_npe[i] = MMD_unweighted(posterior_samples_npe[i], bern_sir_post_reference, lengthscale=median_heuristic(bern_sir_post_reference))\n",
    "    mmd_canpe[i] = MMD_unweighted(posterior_samples_canpe[i], bern_sir_post_reference, lengthscale=median_heuristic(bern_sir_post_reference))\n",
    "    cost_npe[i] = torch.tensor(checkpoint[\"cost_npe\"])\n",
    "    cost_canpe[i] = torch.tensor(checkpoint[\"cost_canpe\"])\n",
    "    \n",
    "    cost_saved[i] = 1 - cost_canpe[i] / cost_npe[i]\n",
    "    \n",
    "mmd_npe = mmd_npe.detach().numpy() \n",
    "mmd_npe_mean = np.mean(mmd_npe)\n",
    "mmd_npe_std = np.std(mmd_npe)\n",
    "\n",
    "mmd_canpe = mmd_canpe.detach().numpy() \n",
    "mmd_canpe_mean = np.mean(mmd_canpe)\n",
    "mmd_canpe_std = np.std(mmd_canpe)\n",
    "\n",
    "cost_npe = cost_npe.detach().numpy() \n",
    "cost_npe_mean = np.mean(cost_npe)\n",
    "cost_npe_std = np.std(cost_npe)\n",
    "\n",
    "cost_canpe = cost_canpe.detach().numpy() \n",
    "cost_canpe_mean = np.mean(cost_canpe)\n",
    "cost_canpe_std = np.std(cost_canpe)\n",
    "\n",
    "cost_saved = cost_saved.detach().numpy()\n",
    "cost_saved_mean = np.mean(cost_saved)\n",
    "cost_saved_std = np.std(cost_saved)\n",
    "\n",
    "print(f\"NPE MMD mean {mmd_npe_mean:.2f} (std {mmd_npe_std:.2f})\")\n",
    "print(f\"CA-NPE MMD mean {mmd_canpe_mean:.2f} (std {mmd_canpe_std:.2f})\")\n",
    "\n",
    "print(f\"NPE cost mean {cost_npe_mean:.2f} (std {cost_npe_std:.2f})\")\n",
    "print(f\"CA-NPE cost mean {cost_canpe_mean:.2f} (std {cost_canpe_std:.2f})\")\n",
    "\n",
    "print(f\"Cost saved: {cost_saved_mean}(std {cost_saved_std})\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "689dc6b6",
   "metadata": {},
   "source": [
    "k = 1.0\n",
    "n_run = 50\n",
    "mmd_npe = torch.zeros([n_run])\n",
    "posterior_samples_npe = torch.zeros([n_run, 1000, 3])\n",
    "cost_npe = torch.zeros([n_run])\n",
    "mmd_canpe = torch.zeros([n_run])\n",
    "posterior_samples_canpe = torch.zeros([n_run, 1000, 3])\n",
    "cost_canpe = torch.zeros([n_run])\n",
    "\n",
    "cost_saved = torch.zeros([n_run])\n",
    "\n",
    "\n",
    "for i in range(n_run):\n",
    "    checkpoint_path = f\"sims/bern_sir/{k}/{i+1}/ckpt.tar\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    \n",
    "    posterior_npe = checkpoint[\"posterior_npe\"]\n",
    "    posterior_canpe = checkpoint[\"posterior_canpe\"]\n",
    "    \n",
    "\n",
    "#     posterior_samples_npe[i] = checkpoint[\"posterior_samples_npe\"]\n",
    "#     posterior_samples_canpe[i] = checkpoint[\"posterior_samples_canpe\"]\n",
    "    \n",
    "    posterior_samples_npe[i] = posterior_npe.sample((1000,), x=bern_sir_obs_x, show_progress_bars=False)\n",
    "    posterior_samples_canpe[i] = posterior_canpe.sample((1000,), x=bern_sir_obs_x, show_progress_bars=False)\n",
    "    \n",
    "    mmd_npe[i] = MMD_unweighted(posterior_samples_npe[i], bern_sir_post_reference, lengthscale=median_heuristic(bern_sir_post_reference))\n",
    "    mmd_canpe[i] = MMD_unweighted(posterior_samples_canpe[i], bern_sir_post_reference, lengthscale=median_heuristic(bern_sir_post_reference))\n",
    "    cost_npe[i] = torch.tensor(checkpoint[\"cost_npe\"])\n",
    "    cost_canpe[i] = torch.tensor(checkpoint[\"cost_canpe\"])\n",
    "    \n",
    "    cost_saved[i] = 1 - cost_canpe[i] / cost_npe[i]\n",
    "    \n",
    "mmd_npe = mmd_npe.detach().numpy() \n",
    "mmd_npe_mean = np.mean(mmd_npe)\n",
    "mmd_npe_std = np.std(mmd_npe)\n",
    "\n",
    "mmd_canpe = mmd_canpe.detach().numpy() \n",
    "mmd_canpe_mean = np.mean(mmd_canpe)\n",
    "mmd_canpe_std = np.std(mmd_canpe)\n",
    "\n",
    "cost_npe = cost_npe.detach().numpy() \n",
    "cost_npe_mean = np.mean(cost_npe)\n",
    "cost_npe_std = np.std(cost_npe)\n",
    "\n",
    "cost_canpe = cost_canpe.detach().numpy() \n",
    "cost_canpe_mean = np.mean(cost_canpe)\n",
    "cost_canpe_std = np.std(cost_canpe)\n",
    "\n",
    "cost_saved = cost_saved.detach().numpy()\n",
    "cost_saved_mean = np.mean(cost_saved)\n",
    "cost_saved_std = np.std(cost_saved)\n",
    "\n",
    "print(f\"NPE MMD mean {mmd_npe_mean:.2f} (std {mmd_npe_std:.2f})\")\n",
    "print(f\"CA-NPE MMD mean {mmd_canpe_mean:.2f} (std {mmd_canpe_std:.2f})\")\n",
    "\n",
    "print(f\"NPE cost mean {cost_npe_mean:.2f} (std {cost_npe_std:.2f})\")\n",
    "print(f\"CA-NPE cost mean {cost_canpe_mean:.2f} (std {cost_canpe_std:.2f})\")\n",
    "\n",
    "print(f\"Cost saved: {cost_saved_mean}(std {cost_saved_std})\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9d51bc60",
   "metadata": {},
   "source": [
    "k = 2.0\n",
    "n_run = 100\n",
    "mmd_npe = torch.zeros([n_run])\n",
    "posterior_samples_npe = torch.zeros([n_run, 1000, 3])\n",
    "cost_npe = torch.zeros([n_run])\n",
    "mmd_canpe = torch.zeros([n_run])\n",
    "posterior_samples_canpe = torch.zeros([n_run, 1000, 3])\n",
    "cost_canpe = torch.zeros([n_run])\n",
    "\n",
    "cost_saved = torch.zeros([n_run])\n",
    "\n",
    "\n",
    "for i in range(n_run):\n",
    "    checkpoint_path = f\"sims/bern_sir/{k}/{i+1}/ckpt.tar\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    \n",
    "    posterior_npe = checkpoint[\"posterior_npe\"]\n",
    "    posterior_canpe = checkpoint[\"posterior_canpe\"]\n",
    "    \n",
    "\n",
    "#     posterior_samples_npe[i] = checkpoint[\"posterior_samples_npe\"]\n",
    "#     posterior_samples_canpe[i] = checkpoint[\"posterior_samples_canpe\"]\n",
    "    \n",
    "    posterior_samples_npe[i] = posterior_npe.sample((1000,), x=bern_sir_obs_x, show_progress_bars=False)\n",
    "    posterior_samples_canpe[i] = posterior_canpe.sample((1000,), x=bern_sir_obs_x, show_progress_bars=False)\n",
    "    \n",
    "    mmd_npe[i] = MMD_unweighted(posterior_samples_npe[i], bern_sir_post_reference, lengthscale=median_heuristic(bern_sir_post_reference))\n",
    "    mmd_canpe[i] = MMD_unweighted(posterior_samples_canpe[i], bern_sir_post_reference, lengthscale=median_heuristic(bern_sir_post_reference))\n",
    "    cost_npe[i] = torch.tensor(checkpoint[\"cost_npe\"])\n",
    "    cost_canpe[i] = torch.tensor(checkpoint[\"cost_canpe\"])\n",
    "    \n",
    "    cost_saved[i] = 1 - cost_canpe[i] / cost_npe[i]\n",
    "    \n",
    "mmd_npe = mmd_npe.detach().numpy() \n",
    "mmd_npe_mean = np.mean(mmd_npe)\n",
    "mmd_npe_std = np.std(mmd_npe)\n",
    "\n",
    "mmd_canpe = mmd_canpe.detach().numpy() \n",
    "mmd_canpe_mean = np.mean(mmd_canpe)\n",
    "mmd_canpe_std = np.std(mmd_canpe)\n",
    "\n",
    "cost_npe = cost_npe.detach().numpy() \n",
    "cost_npe_mean = np.mean(cost_npe)\n",
    "cost_npe_std = np.std(cost_npe)\n",
    "\n",
    "cost_canpe = cost_canpe.detach().numpy() \n",
    "cost_canpe_mean = np.mean(cost_canpe)\n",
    "cost_canpe_std = np.std(cost_canpe)\n",
    "\n",
    "cost_saved = cost_saved.detach().numpy()\n",
    "cost_saved_mean = np.mean(cost_saved)\n",
    "cost_saved_std = np.std(cost_saved)\n",
    "\n",
    "print(f\"NPE MMD mean {mmd_npe_mean:.2f} (std {mmd_npe_std:.2f})\")\n",
    "print(f\"CA-NPE MMD mean {mmd_canpe_mean:.2f} (std {mmd_canpe_std:.2f})\")\n",
    "\n",
    "print(f\"NPE cost mean {cost_npe_mean:.2f} (std {cost_npe_std:.2f})\")\n",
    "print(f\"CA-NPE cost mean {cost_canpe_mean:.2f} (std {cost_canpe_std:.2f})\")\n",
    "\n",
    "print(f\"Cost saved: {cost_saved_mean}(std {cost_saved_std})\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5c2f6ead",
   "metadata": {},
   "source": [
    "seed = 1\n",
    "plt.figure(figsize=[17,5])\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.xlim(0, 1)\n",
    "sns.kdeplot(bern_sir_post_reference[:,0], color = \"C1\", linewidth = 2, linestyle = \"solid\", label = \"NPE (reference)\")\n",
    "sns.kdeplot(posterior_samples_canpe[seed,:,0], color = \"C2\", linewidth = 2, linestyle = \"solid\", label = \"CA-NPE\")\n",
    "sns.kdeplot(posterior_samples_npe[seed,:,0], color = \"C4\", linewidth = 2, linestyle = \"solid\", label = \"NPE\")\n",
    "plt.axvline(x=bern_sir_obs_theta.detach().numpy()[0], color='r', linestyle='--', linewidth=2, label='$\\\\theta_{true}$')\n",
    "\n",
    "plt.xlabel(\"$\\\\beta$\", fontsize = 20)\n",
    "plt.ylabel(\"Density\", fontsize=20)\n",
    "# plt.title(f\"MMD:{mmd[seed]:.2f}\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.xlim(0, 1)\n",
    "sns.kdeplot(bern_sir_post_reference[:,1], color = \"C1\", linewidth = 2, linestyle = \"solid\", label = \"NPE (reference)\")\n",
    "sns.kdeplot(posterior_samples_canpe[seed,:,1], color = \"C2\", linewidth = 2, linestyle = \"solid\", label = \"CA-NPE\")\n",
    "sns.kdeplot(posterior_samples_npe[seed,:,1], color = \"C4\", linewidth = 2, linestyle = \"solid\", label = \"NPE\")\n",
    "plt.axvline(x=bern_sir_obs_theta.detach().numpy()[1], color='r', linestyle='--', linewidth=2, label='$\\\\theta_{true}$')\n",
    "plt.xlabel(\"$\\\\gamma$\", fontsize = 20)\n",
    "plt.ylabel(\"Density\", fontsize=20)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.xlim(0, 1)\n",
    "sns.kdeplot(bern_sir_post_reference[:,2], color = \"C1\", linewidth = 2, linestyle = \"solid\", label = \"NPE (reference)\")\n",
    "sns.kdeplot(posterior_samples_canpe[seed,:,2], color = \"C2\", linewidth = 2, linestyle = \"solid\", label = \"CA-NPE\")\n",
    "sns.kdeplot(posterior_samples_npe[seed,:,2], color = \"C4\", linewidth = 2, linestyle = \"solid\", label = \"NPE\")\n",
    "plt.axvline(x=bern_sir_obs_theta.detach().numpy()[2], color='r', linestyle='--', linewidth=2, label='$\\\\theta_{true}$')\n",
    "plt.legend(fontsize=20, loc=1)\n",
    "plt.xlabel(\"$p$\", fontsize = 20)\n",
    "plt.ylabel(\"Density\", fontsize=20)\n",
    "\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "99415886",
   "metadata": {},
   "source": [
    "# Cost plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9c7a020",
   "metadata": {},
   "source": [
    "N = 20\n",
    "\n",
    "beta = torch.linspace(0.1, 1, N)\n",
    "beta_fixed = 0.5\n",
    "\n",
    "\n",
    "gamma_fixed = 0.5\n",
    "gamma = torch.linspace(0.1, 1, N)\n",
    "\n",
    "p = torch.linspace(0.1, 1, N)\n",
    "p_fixed = 0.5\n",
    "\n",
    "beta_combined = beta_fixed * torch.ones_like(beta)\n",
    "gamma_combined = gamma_fixed * torch.ones_like(gamma)\n",
    "p_combined = p_fixed * torch.ones_like(p)\n",
    "\n",
    "combination_beta = torch.stack((beta, gamma_combined, p_combined), dim=1)\n",
    "combination_gamma = torch.stack((beta_combined, gamma, p_combined), dim=1)\n",
    "combination_p = torch.stack((beta_combined, gamma_combined, p), dim=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97ec5b46",
   "metadata": {},
   "source": [
    "plt.figure(figsize=[17,5])\n",
    "# plt.suptitle(\"     Bernoulli SIR\", fontsize=20)\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(beta, times_vs_beta)\n",
    "\n",
    "plt.xlabel(\"$\\\\beta$: infection rate\", fontsize=20)\n",
    "plt.ylabel(\"Cost [seconds]\", fontsize=20)\n",
    "# plt.grid()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(gamma, times_vs_gamma)\n",
    "\n",
    "plt.xlabel(\"$\\\\gamma$: removal rate\", fontsize=20)\n",
    "\n",
    "# plt.grid()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(p, times_vs_p)\n",
    "\n",
    "plt.xlabel(\"$p$: edge probability\", fontsize=20)\n",
    "\n",
    "# plt.grid()\n",
    "\n",
    "plt.savefig(\"plot_cost_bern_sir.pdf\")\n",
    "plt.show()"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
