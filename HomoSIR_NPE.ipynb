{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed06cb3d",
   "metadata": {},
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sbi import utils as utils\n",
    "from sbi import analysis as analysis\n",
    "from cost_aware_snpe_c import CostAwareSNPE_C\n",
    "from sbi.inference.snpe.snpe_c import SNPE_C\n",
    "from cost_aware_snle_a import CostAwareSNLE_A\n",
    "from sbi.utils.torchutils import *\n",
    "from sbi.utils import process_prior\n",
    "from sbi.utils.user_input_checks import *\n",
    "from simulators import homogeneous_sir\n",
    "\n",
    "from hydra import compose, initialize\n",
    "import hydra\n",
    "\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize(config_path=\"configs\", version_base=None)\n",
    "cfg = compose(config_name=\"train\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77dc6ef3",
   "metadata": {},
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams.update({\n",
    "    'font.family' : 'times',\n",
    "    'font.size' : 14.0,\n",
    "    'lines.linewidth' : 2,\n",
    "    'lines.antialiased' : True,\n",
    "    'axes.facecolor': 'fdfdfd',\n",
    "    'axes.edgecolor': '777777',\n",
    "    'axes.linewidth' : 1,\n",
    "    'axes.titlesize' : 'medium',\n",
    "    'axes.labelsize' : 'medium',\n",
    "    'axes.axisbelow' : True,\n",
    "    'xtick.major.size'     : 0,      # major tick size in points\n",
    "    'xtick.minor.size'     : 0,      # minor tick size in points\n",
    "    'xtick.major.pad'      : 6,      # distance to major tick label in points\n",
    "    'xtick.minor.pad'      : 6,      # distance to the minor tick label in points\n",
    "    'xtick.color'          : '333333', # color of the tick labels\n",
    "    'xtick.labelsize'      : 'medium', # fontsize of the tick labels\n",
    "    'xtick.direction'      : 'in',     # direction: in or out\n",
    "    'ytick.major.size'     : 0,      # major tick size in points\n",
    "    'ytick.minor.size'     : 0,      # minor tick size in points\n",
    "    'ytick.major.pad'      : 6,      # distance to major tick label in points\n",
    "    'ytick.minor.pad'      : 6,      # distance to the minor tick label in points\n",
    "    'ytick.color'          : '333333', # color of the tick labels\n",
    "    'ytick.labelsize'      : 'medium', # fontsize of the tick labels\n",
    "    'ytick.direction'      : 'in',     # direction: in or out\n",
    "    'axes.grid' : False,\n",
    "    'grid.alpha' : 0.3,\n",
    "    'grid.linewidth' : 1,\n",
    "    'legend.fancybox' : True,\n",
    "    'legend.fontsize' : 'Small',\n",
    "    'figure.figsize' : (2.5, 2.5),\n",
    "    'figure.facecolor' : '1.0',\n",
    "    'figure.edgecolor' : '0.5',\n",
    "    'hatch.linewidth' : 0.1,\n",
    "    'text.usetex' : True\n",
    "    })\n",
    "\n",
    "color_map = {'green': '#009E60', 'orange': '#C04000',\n",
    "              'blue': '#00416A', 'black':'#3A3B3C',\n",
    "              'purple': '#843B62', 'red': '#C41E3A'}\n",
    "\n",
    "\n",
    "plt.rcParams['text.latex.preamble'] = r'\\usepackage{times}'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b9f2164",
   "metadata": {},
   "source": [
    "def MMD_unweighted(x, y, lengthscale):\n",
    "    \"\"\" Approximates the squared MMD between samples x_i ~ P and y_i ~ Q\n",
    "    \"\"\"\n",
    "\n",
    "    m = x.shape[0]\n",
    "    n = y.shape[0]\n",
    "\n",
    "    z = torch.cat((x, y), dim=0)\n",
    "\n",
    "    K = kernel_matrix(z, z, lengthscale)\n",
    "\n",
    "    kxx = K[0:m, 0:m]\n",
    "    kyy = K[m:(m + n), m:(m + n)]\n",
    "    kxy = K[0:m, m:(m + n)]\n",
    "\n",
    "    return (1 / m ** 2) * torch.sum(kxx) - (2 / (m * n)) * torch.sum(kxy) + (1 / n ** 2) * torch.sum(kyy)\n",
    "\n",
    "\n",
    "def median_heuristic(y):\n",
    "    a = torch.cdist(y, y)**2\n",
    "    return torch.sqrt(torch.median(a / 2))\n",
    "\n",
    "\n",
    "def kernel_matrix(x, y, l):\n",
    "    d = torch.cdist(x, y)**2\n",
    "\n",
    "    kernel = torch.exp(-(1 / (2 * l ** 2)) * d)\n",
    "\n",
    "    return kernel"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f456b12e",
   "metadata": {},
   "source": [
    "def calc_acc_prob(gp, likelihood, theta, prior_start, k):\n",
    "\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        cost = likelihood(model(theta)).mean\n",
    "\n",
    "        lower_cost = likelihood(model(prior_start)).mean\n",
    "    return (lower_cost ** k) / (cost**k)\n",
    "\n",
    "class GP(ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GP, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = ConstantMean()\n",
    "        self.covar_module = ScaleKernel(RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x, covar_x)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5328cdea",
   "metadata": {},
   "source": [
    "# Homo SIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79b79715",
   "metadata": {},
   "source": [
    "homo_sir = homogeneous_sir.HomoSIR()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c5d50e97",
   "metadata": {},
   "source": [
    "## data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "18d9ab59",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# DON'T NEED TO RUN\n",
    "N = 50000\n",
    "\n",
    "homo_sir_theta_npe_large = homo_sir.sample_theta([N]).reshape(-1, 1)\n",
    "homo_sir_x_npe_large = torch.empty([N, 1])\n",
    "for i in range(N):\n",
    "    homo_sir_x_npe_large[i, :] = homo_sir(homo_sir_theta_npe_large[i])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dc95fb2b",
   "metadata": {},
   "source": [
    "# Use triton to generate data\n",
    "N = 50000\n",
    "\n",
    "homo_sir_theta_npe_large = torch.empty([N, homo_sir.theta_dim])\n",
    "homo_sir_x_npe_large = torch.empty([N, homo_sir.x_dim])\n",
    "for i in range(100):\n",
    "    homo_sir_x_npe_large[500*i: 500*(i+1), :] = torch.load(f\"data/homo_sir_large/homo_sir_x_npe_large_{i+1}.pt\")\n",
    "    homo_sir_theta_npe_large[500*i: 500*(i+1), :] = torch.load(f\"data/homo_sir_large/homo_sir_theta_npe_large_{i+1}.pt\")\n",
    "\n",
    "torch.save(homo_sir_x_npe_large, \"data/homo_sir_x_npe_large.pt\")\n",
    "torch.save(homo_sir_theta_npe_large, \"data/homo_sir_theta_npe_large.pt\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4e981d0f",
   "metadata": {},
   "source": [
    "# DON'T NEED TO RUN\n",
    "\n",
    "# Observed data generation\n",
    "homo_sir_obs_theta = torch.tensor([5])\n",
    "homo_sir_obs_x = homo_sir(homo_sir_obs_theta)\n",
    "homo_sir_obs_x"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "76a2b989",
   "metadata": {},
   "source": [
    "## data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ceba0527",
   "metadata": {},
   "source": [
    "# DON'T NEED TO RUN\n",
    "# torch.save(homo_sir_x_npe_large, \"data/homo_sir_x_npe_large.pt\")\n",
    "# torch.save(homo_sir_theta_npe_large, \"data/homo_sir_theta_npe_large.pt\")\n",
    "\n",
    "# torch.save(homo_sir_obs_x, \"data/homo_sir_obs_x.pt\")\n",
    "# torch.save(homo_sir_obs_theta, \"data/homo_sir_obs_theta.pt\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44965348",
   "metadata": {},
   "source": [
    "homo_sir_x_npe_large = torch.load(\"data/homo_sir_x_npe_large.pt\")\n",
    "homo_sir_theta_npe_large = torch.load(\"data/homo_sir_theta_npe_large.pt\")\n",
    "\n",
    "homo_sir_obs_x = torch.load(\"data/homo_sir_obs_x.pt\")\n",
    "homo_sir_obs_theta = torch.load(\"data/homo_sir_obs_theta.pt\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9933f1a2",
   "metadata": {},
   "source": [
    "## Fit GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3eea1fac",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# for gp\n",
    "n_train_pair = 100\n",
    "X = homo_sir_theta_npe_large[:n_train_pair]\n",
    "n_rep = 20\n",
    "\n",
    "times_train = torch.zeros(n_train_pair)\n",
    "\n",
    "for i in range(n_train_pair):\n",
    "    st = time.time()\n",
    "    for _ in range(n_rep):\n",
    "        result = homo_sir(homo_sir_theta_npe_large[i])\n",
    "    et = time.time()\n",
    "    times_train[i] = (et - st)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a934a8c5",
   "metadata": {},
   "source": [
    "# save GP training samples\n",
    "torch.save(X, \"data/homo_sir_gp_x.pt\")\n",
    "torch.save(times_train, \"data/homo_sir_gp_y.pt\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6eec359",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = GP(X, times_train, likelihood)\n",
    "model.float()\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "mll = ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iterations = 50\n",
    "for i in range(training_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X.float())\n",
    "    loss = -mll(output, times_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Iter {i + 1}/{training_iterations} - Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'likelihood_state_dict': likelihood.state_dict()\n",
    "}, 'data/homo_sir_gp.pth')\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e890d902",
   "metadata": {},
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "x_test = torch.linspace(1, 10, 100).reshape(-1, 1)\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    observed_pred = likelihood(model(x_test)).mean\n",
    "\n",
    "plt.figure(figsize=[5,5])\n",
    "plt.scatter(x_test, observed_pred, label=\"GP\")\n",
    "plt.scatter(X, times_train, label=\"True\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80bbb1c8",
   "metadata": {},
   "source": [
    "state_dicts = torch.load('data/homo_sir_gp.pth')\n",
    "\n",
    "X = torch.load(\"data/homo_sir_gp_x.pt\")\n",
    "times_train = torch.load(\"data/homo_sir_gp_y.pt\")\n",
    "\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = GP(X, times_train, likelihood)\n",
    "model.float()\n",
    "\n",
    "model.load_state_dict(state_dicts['model_state_dict'])\n",
    "likelihood.load_state_dict(state_dicts['likelihood_state_dict'])\n",
    "\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "prior_start = torch.tensor([1.]).reshape(-1, 1)\n",
    "k = 1\n",
    "\n",
    "num_sim = 5000\n",
    "theta_tilde = torch.zeros([num_sim, 1])\n",
    "count = 0\n",
    "while count < num_sim:\n",
    "    theta = homo_sir.sample_theta([1]).reshape(-1, 1)\n",
    "    if calc_acc_prob(model, likelihood, theta, prior_start, k) > torch.rand(1):\n",
    "        theta_tilde[count] = theta.reshape(-1)\n",
    "        count += 1\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c37a9737",
   "metadata": {},
   "source": [
    "w = likelihood(model(theta_tilde)).mean.detach() ** k"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e63e3a63",
   "metadata": {},
   "source": [
    "plt.hist(theta_tilde.detach().numpy(), bins=10)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2b316a4b",
   "metadata": {},
   "source": [
    "x_npe = torch.empty([num_sim, 1])\n",
    "for i in range(num_sim):\n",
    "    x_npe[i, :] = homo_sir(theta_tilde[i])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8fb14aec",
   "metadata": {},
   "source": [
    "homo_sir_inference_canpe = CostAwareSNPE_C()\n",
    "homo_sir_nn_canpe = homo_sir_inference_canpe.append_simulations(\n",
    "    theta_tilde, x_npe).append_weights(w).train()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "06adbbb7",
   "metadata": {},
   "source": [
    "# DON'T NEED TO RUN\n",
    "homo_sir_post_canpe = homo_sir_inference_canpe.build_posterior(homo_sir_nn_canpe, prior=homo_sir.prior)\n",
    "homo_sir_samples_canpe = homo_sir_post_canpe.sample((1000,), x=homo_sir_obs_x)\n",
    "\n",
    "# torch.save(homo_sir_samples_npe_large, \"data/homo_sir_post_reference.pt\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6c5c47b4",
   "metadata": {},
   "source": [
    "## inference NPE large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "36986438",
   "metadata": {},
   "source": [
    "# DON'T NEED TO RUN\n",
    "homo_sir_inference_npe_large = SNPE_C()\n",
    "homo_sir_nn_npe_large = homo_sir_inference_npe_large.append_simulations(\n",
    "    homo_sir_theta_npe_large, homo_sir_x_npe_large).train()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e02dc961",
   "metadata": {},
   "source": [
    "# DON'T NEED TO RUN\n",
    "homo_sir_post_npe_large = homo_sir_inference_npe_large.build_posterior(homo_sir_nn_npe_large, prior=homo_sir.prior)\n",
    "homo_sir_samples_npe_large = homo_sir_post_npe_large.sample((1000,), x=homo_sir_obs_x)\n",
    "\n",
    "torch.save(homo_sir_samples_npe_large, \"data/homo_sir_post_reference.pt\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f705ecb4",
   "metadata": {},
   "source": [
    "homo_sir_post_reference = torch.load(\"data/homo_sir_post_reference.pt\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d4fce1a9",
   "metadata": {},
   "source": [
    "## CEG ESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8e15f96",
   "metadata": {},
   "source": [
    "num_sim = 2000\n",
    "num_repeats = 1\n",
    "k = np.array([0, 0.2, 0.4, 0.5, 1.0, 2.0]) # Exponent of the penaly function g(z) = z^k\n",
    "\n",
    "ess_cost_aware = np.zeros([k.size, num_repeats])\n",
    "ceg = np.zeros([k.size, num_repeats])\n",
    "\n",
    "for ind in range(k.size):\n",
    "    for j in range(num_repeats):\n",
    "        \n",
    "        if ind == 0:\n",
    "            theta = homo_sir.sample_theta([num_sim])\n",
    "            ess_cost_aware[ind, j] = 1\n",
    "            ceg[ind, j] = 1\n",
    "        else:\n",
    "            # Sampling from cost-modified prior\n",
    "            theta_tilde = torch.zeros([num_sim, 1])\n",
    "            count = 0\n",
    "            while count < num_sim:\n",
    "                param_value = homo_sir.sample_theta([1]).reshape(-1, 1)\n",
    "                if calc_acc_prob(model, likelihood, param_value, prior_start, k[ind]) > torch.rand(1):\n",
    "                    theta_tilde[count] = param_value.reshape(-1)\n",
    "                    count += 1\n",
    "\n",
    "            w_u = likelihood(model(theta_tilde)).mean.detach() ** k[ind]   #self-normalised importance weights\n",
    "            \n",
    "            # Compute CEG\n",
    "            ceg[ind, j] = torch.mean(likelihood(model(theta)).mean.detach()) / torch.mean(likelihood(model(theta_tilde)).mean.detach())\n",
    "            # Compute ESS\n",
    "            ess_cost_aware[ind, j] = ((w_u.sum())**2 / torch.square(w_u).sum()) / num_sim\n",
    "    print(ind)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fcd31a1",
   "metadata": {},
   "source": [
    "ess_cost_aware * ceg"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "612d6161",
   "metadata": {},
   "source": [
    "## Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "90b3d702",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "k = 0.5\n",
    "n_run = 100\n",
    "mmd_npe = torch.zeros([n_run])\n",
    "posterior_samples_npe = torch.zeros([n_run, 1000, 1])\n",
    "cost_npe = torch.zeros([n_run])\n",
    "mmd_canpe = torch.zeros([n_run])\n",
    "posterior_samples_canpe = torch.zeros([n_run, 1000, 1])\n",
    "cost_canpe = torch.zeros([n_run])\n",
    "\n",
    "cost_saved = torch.zeros([n_run])\n",
    "\n",
    "\n",
    "for i in range(n_run):\n",
    "    checkpoint_path = f\"sims/homo_sir/{k}/{i+1}/ckpt.tar\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "\n",
    "    posterior_npe = checkpoint[\"posterior_npe\"]\n",
    "    posterior_canpe = checkpoint[\"posterior_canpe\"]\n",
    "    \n",
    "#     posterior_samples_npe[i] = checkpoint[\"posterior_samples_npe\"]\n",
    "#     posterior_samples_canpe[i] = checkpoint[\"posterior_samples_canpe\"]\n",
    "    \n",
    "    posterior_samples_npe[i] = posterior_npe.sample((1000,), x=homo_sir_obs_x, show_progress_bars=False)\n",
    "    posterior_samples_canpe[i] = posterior_canpe.sample((1000,), x=homo_sir_obs_x, show_progress_bars=False)\n",
    "    \n",
    "    mmd_npe[i] = MMD_unweighted(posterior_samples_npe[i], homo_sir_post_reference, lengthscale=median_heuristic(homo_sir_post_reference))\n",
    "    mmd_canpe[i] = MMD_unweighted(posterior_samples_canpe[i], homo_sir_post_reference, lengthscale=median_heuristic(homo_sir_post_reference))\n",
    "    cost_npe[i] = torch.tensor(checkpoint[\"cost_npe\"])\n",
    "    cost_canpe[i] = torch.tensor(checkpoint[\"cost_canpe\"])\n",
    "    \n",
    "    cost_saved[i] = 1 - cost_canpe[i] / cost_npe[i]\n",
    "    \n",
    "mmd_npe = mmd_npe.detach().numpy() \n",
    "mmd_npe_mean = np.mean(mmd_npe)\n",
    "mmd_npe_std = np.std(mmd_npe)\n",
    "\n",
    "mmd_canpe = mmd_canpe.detach().numpy() \n",
    "mmd_canpe_mean = np.mean(mmd_canpe)\n",
    "mmd_canpe_std = np.std(mmd_canpe)\n",
    "\n",
    "cost_npe = cost_npe.detach().numpy() \n",
    "cost_npe_mean = np.mean(cost_npe)\n",
    "cost_npe_std = np.std(cost_npe)\n",
    "\n",
    "cost_canpe = cost_canpe.detach().numpy() \n",
    "cost_canpe_mean = np.mean(cost_canpe)\n",
    "cost_canpe_std = np.std(cost_canpe)\n",
    "\n",
    "cost_saved = cost_saved.detach().numpy()\n",
    "cost_saved_mean = np.mean(cost_saved)\n",
    "cost_saved_std = np.std(cost_saved)\n",
    "\n",
    "print(f\"NPE MMD mean {mmd_npe_mean:.2f} (std {mmd_npe_std:.2f})\")\n",
    "print(f\"CA-NPE MMD mean {mmd_canpe_mean:.2f} (std {mmd_canpe_std:.2f})\")\n",
    "\n",
    "print(f\"NPE cost mean {cost_npe_mean:.2f} (std {cost_npe_std:.2f})\")\n",
    "print(f\"CA-NPE cost mean {cost_canpe_mean:.2f} (std {cost_canpe_std:.2f})\")\n",
    "\n",
    "print(f\"Cost saved: {cost_saved_mean}(std {cost_saved_std})\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "7d298a29",
   "metadata": {},
   "source": [
    "k = 1.0\n",
    "n_run = 100\n",
    "mmd_npe = torch.zeros([n_run])\n",
    "posterior_samples_npe = torch.zeros([n_run, 1000, 1])\n",
    "cost_npe = torch.zeros([n_run])\n",
    "mmd_canpe = torch.zeros([n_run])\n",
    "posterior_samples_canpe = torch.zeros([n_run, 1000, 1])\n",
    "cost_canpe = torch.zeros([n_run])\n",
    "\n",
    "cost_saved = torch.zeros([n_run])\n",
    "\n",
    "\n",
    "for i in range(n_run):\n",
    "    checkpoint_path = f\"sims/homo_sir/{k}/{i+1}/ckpt.tar\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "\n",
    "    posterior_npe = checkpoint[\"posterior_npe\"]\n",
    "    posterior_canpe = checkpoint[\"posterior_canpe\"]\n",
    "    \n",
    "#     posterior_samples_npe[i] = checkpoint[\"posterior_samples_npe\"]\n",
    "#     posterior_samples_canpe[i] = checkpoint[\"posterior_samples_canpe\"]\n",
    "    \n",
    "    posterior_samples_npe[i] = posterior_npe.sample((1000,), x=homo_sir_obs_x, show_progress_bars=False)\n",
    "    posterior_samples_canpe[i] = posterior_canpe.sample((1000,), x=homo_sir_obs_x, show_progress_bars=False)\n",
    "    \n",
    "    mmd_npe[i] = MMD_unweighted(posterior_samples_npe[i], homo_sir_post_reference, lengthscale=median_heuristic(homo_sir_post_reference))\n",
    "    mmd_canpe[i] = MMD_unweighted(posterior_samples_canpe[i], homo_sir_post_reference, lengthscale=median_heuristic(homo_sir_post_reference))\n",
    "    cost_npe[i] = torch.tensor(checkpoint[\"cost_npe\"])\n",
    "    cost_canpe[i] = torch.tensor(checkpoint[\"cost_canpe\"])\n",
    "    \n",
    "    cost_saved[i] = 1 - cost_canpe[i] / cost_npe[i]\n",
    "    \n",
    "mmd_npe = mmd_npe.detach().numpy() \n",
    "mmd_npe_mean = np.mean(mmd_npe)\n",
    "mmd_npe_std = np.std(mmd_npe)\n",
    "\n",
    "mmd_canpe = mmd_canpe.detach().numpy() \n",
    "mmd_canpe_mean = np.mean(mmd_canpe)\n",
    "mmd_canpe_std = np.std(mmd_canpe)\n",
    "\n",
    "cost_npe = cost_npe.detach().numpy() \n",
    "cost_npe_mean = np.mean(cost_npe)\n",
    "cost_npe_std = np.std(cost_npe)\n",
    "\n",
    "cost_canpe = cost_canpe.detach().numpy() \n",
    "cost_canpe_mean = np.mean(cost_canpe)\n",
    "cost_canpe_std = np.std(cost_canpe)\n",
    "\n",
    "cost_saved = cost_saved.detach().numpy()\n",
    "cost_saved_mean = np.mean(cost_saved)\n",
    "cost_saved_std = np.std(cost_saved)\n",
    "\n",
    "print(f\"NPE MMD mean {mmd_npe_mean:.2f} (std {mmd_npe_std:.2f})\")\n",
    "print(f\"CA-NPE MMD mean {mmd_canpe_mean:.2f} (std {mmd_canpe_std:.2f})\")\n",
    "\n",
    "print(f\"NPE cost mean {cost_npe_mean:.2f} (std {cost_npe_std:.2f})\")\n",
    "print(f\"CA-NPE cost mean {cost_canpe_mean:.2f} (std {cost_canpe_std:.2f})\")\n",
    "\n",
    "print(f\"Cost saved: {cost_saved_mean}(std {cost_saved_std})\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "a8bcd572",
   "metadata": {},
   "source": [
    "k = 2.0\n",
    "n_run = 50\n",
    "mmd_npe = torch.zeros([n_run])\n",
    "posterior_samples_npe = torch.zeros([n_run, 1000, 1])\n",
    "cost_npe = torch.zeros([n_run])\n",
    "mmd_canpe = torch.zeros([n_run])\n",
    "posterior_samples_canpe = torch.zeros([n_run, 1000, 1])\n",
    "cost_canpe = torch.zeros([n_run])\n",
    "\n",
    "cost_saved = torch.zeros([n_run])\n",
    "\n",
    "\n",
    "for i in range(n_run):\n",
    "    checkpoint_path = f\"sims/homo_sir/{k}/{i+1}/ckpt.tar\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "\n",
    "    posterior_npe = checkpoint[\"posterior_npe\"]\n",
    "    posterior_canpe = checkpoint[\"posterior_canpe\"]\n",
    "    \n",
    "#     posterior_samples_npe[i] = checkpoint[\"posterior_samples_npe\"]\n",
    "#     posterior_samples_canpe[i] = checkpoint[\"posterior_samples_canpe\"]\n",
    "    \n",
    "    posterior_samples_npe[i] = posterior_npe.sample((1000,), x=homo_sir_obs_x, show_progress_bars=False)\n",
    "    posterior_samples_canpe[i] = posterior_canpe.sample((1000,), x=homo_sir_obs_x, show_progress_bars=False)\n",
    "    \n",
    "    mmd_npe[i] = MMD_unweighted(posterior_samples_npe[i], homo_sir_post_reference, lengthscale=median_heuristic(homo_sir_post_reference))\n",
    "    mmd_canpe[i] = MMD_unweighted(posterior_samples_canpe[i], homo_sir_post_reference, lengthscale=median_heuristic(homo_sir_post_reference))\n",
    "    cost_npe[i] = torch.tensor(checkpoint[\"cost_npe\"])\n",
    "    cost_canpe[i] = torch.tensor(checkpoint[\"cost_canpe\"])\n",
    "    \n",
    "    cost_saved[i] = 1 - cost_canpe[i] / cost_npe[i]\n",
    "    \n",
    "mmd_npe = mmd_npe.detach().numpy() \n",
    "mmd_npe_mean = np.mean(mmd_npe)\n",
    "mmd_npe_std = np.std(mmd_npe)\n",
    "\n",
    "mmd_canpe = mmd_canpe.detach().numpy() \n",
    "mmd_canpe_mean = np.mean(mmd_canpe)\n",
    "mmd_canpe_std = np.std(mmd_canpe)\n",
    "\n",
    "cost_npe = cost_npe.detach().numpy() \n",
    "cost_npe_mean = np.mean(cost_npe)\n",
    "cost_npe_std = np.std(cost_npe)\n",
    "\n",
    "cost_canpe = cost_canpe.detach().numpy() \n",
    "cost_canpe_mean = np.mean(cost_canpe)\n",
    "cost_canpe_std = np.std(cost_canpe)\n",
    "\n",
    "cost_saved = cost_saved.detach().numpy()\n",
    "cost_saved_mean = np.mean(cost_saved)\n",
    "cost_saved_std = np.std(cost_saved)\n",
    "\n",
    "print(f\"NPE MMD mean {mmd_npe_mean:.2f} (std {mmd_npe_std:.2f})\")\n",
    "print(f\"CA-NPE MMD mean {mmd_canpe_mean:.2f} (std {mmd_canpe_std:.2f})\")\n",
    "\n",
    "print(f\"NPE cost mean {cost_npe_mean:.2f} (std {cost_npe_std:.2f})\")\n",
    "print(f\"CA-NPE cost mean {cost_canpe_mean:.2f} (std {cost_canpe_std:.2f})\")\n",
    "\n",
    "print(f\"Cost saved: {cost_saved_mean}(std {cost_saved_std})\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "8aee5a42",
   "metadata": {},
   "source": [
    "seed = 49\n",
    "plt.figure(figsize=[8,5])\n",
    "plt.xlim(0, 10)\n",
    "sns.kdeplot(homo_sir_post_reference[:,0], color = \"C1\", linewidth = 2, linestyle = \"solid\", label = \"NPE (reference)\")\n",
    "sns.kdeplot(posterior_samples_canpe[seed,:,0], color = \"C2\", linewidth = 2, linestyle = \"solid\", label = \"CA-NPE\")\n",
    "sns.kdeplot(posterior_samples_npe[seed,:,0], color = \"C4\", linewidth = 2, linestyle = \"solid\", label = \"NPE\")\n",
    "plt.axvline(x=homo_sir_obs_theta.detach().numpy(), color='r', linestyle='--', linewidth=2, label='$\\\\theta_{true}$')\n",
    "plt.legend(fontsize=20)\n",
    "plt.xlabel(\"$\\\\theta$\", fontsize = 20)\n",
    "plt.ylabel(\"Density\", fontsize=20)\n",
    "# plt.title(f\"MMD:{mmd[seed]:.2f}\")\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cbb0127b",
   "metadata": {},
   "source": [
    "# Cost plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97ec5b46",
   "metadata": {},
   "source": [
    "n_rep = 50\n",
    "\n",
    "lamb = torch.arange(1, 10, 0.3)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ddf40f1",
   "metadata": {},
   "source": [
    "N = lamb.shape[0]\n",
    "homo_lamb = torch.zeros(N)\n",
    "for i in range(N):\n",
    "    print(\"progress: \", i)\n",
    "    st = time.time()\n",
    "    for _ in range(n_rep):\n",
    "        result = homo_sir(lamb[i].reshape(-1))\n",
    "    et = time.time()\n",
    "    homo_lamb[i] = (et - st) / n_rep\n",
    "\n",
    "print(homo_lamb)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f21e12e4",
   "metadata": {},
   "source": [
    "# Plotting figure\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(lamb, homo_lamb, s=50)\n",
    "# plt.suptitle(\"        Homogeneous SIR\", fontsize=20)\n",
    "plt.xlabel(\"$\\\\lambda$: infection rate\", fontsize=20)\n",
    "plt.ylabel(\"Cost [seconds]\", fontsize=20)\n",
    "# plt.legend()\n",
    "plt.xticks([1, 3, 5, 7, 9])\n",
    "\n",
    "# plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plot_cost_homo_sir.pdf\")\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957aedfc",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
